{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAPDP论文网络复现\n",
    "\n",
    "- [paired-context-embedding](#paired-context-embedding)\n",
    "- [context-encoding](#context-encoding)\n",
    "- [cooperative-multi-agent-decoders](#cooperative-multi-agent-decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/miniconda3/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.12/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch版本:  2.4.0\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "print('torch版本: ', torch.__version__)\n",
    "\n",
    "# 检测GPU，cuda、mps、cpu\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired Context Embedding\n",
    "\n",
    "Embeding是NLP领域的一个概念，比如我们经常会对每一个文本token（单词）做词嵌入，因为深度学习模型的输入是数值张量，词嵌入本质上就是把文本变成向量。本文借鉴了词嵌入的概念，对MAPDP问题的输入节点信息做词嵌入，因为取件节点和接收节点是成对的，而且智能体在选择成对节点时是有时序约束的，为了让模型学习到这种节点（状态）之间的依赖关系，故做嵌入操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.4881,  7.1519, 77.0000],\n",
      "        [ 6.0276,  5.4488, 72.0000],\n",
      "        [ 4.2365,  6.4589,  9.0000],\n",
      "        [ 4.3759,  8.9177, 20.0000],\n",
      "        [ 9.6366,  3.8344, 80.0000],\n",
      "        [ 7.9173,  5.2889, 69.0000],\n",
      "        [ 5.6804,  9.2560, 79.0000]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "NODE_NUM = 7 # 所有节点数\n",
    "N = 3 # 节点对数\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 设置随机种子以确保结果可重复\n",
    "np.random.seed(0)\n",
    "\n",
    "def init_env(n=7):\n",
    "    \"\"\"初始化节点信息\"\"\"\n",
    "    # 生成6个随机坐标，范围在0到10之间\n",
    "    coordinates = np.random.uniform(low=0, high=10, size=(n, 2))\n",
    "\n",
    "    # 生成6个随机容量，范围在0到100之间\n",
    "    capacities = np.random.randint(low=0, high=101, size=n)\n",
    "\n",
    "    # 将坐标和容量组合成一个矩阵\n",
    "    matrix = np.hstack((coordinates, capacities[:, np.newaxis]))\n",
    "\n",
    "    return torch.tensor(matrix ,dtype=torch.float32).to(device)\n",
    "\n",
    "node_information = init_env(NODE_NUM)\n",
    "print(node_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PairedContextEmbedding(\n",
      "  (layer1): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (layer3): Linear(in_features=256, out_features=128, bias=True)\n",
      ")\n",
      "torch.Size([7, 128])\n",
      "tensor([  2.8656,   5.9786,  -4.8060,   6.5953,   0.0620,   1.8579,   4.8849,\n",
      "         -1.5698, -12.6397,  14.8208], device='mps:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PairedContextEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    PairedContextEmbeding层\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(N, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # input是节点信息向量，[7, 3]\n",
    "        x = self.layer1(input)\n",
    "        # 分段Embeding\n",
    "        # i=0\n",
    "        h0_1 = self.layer2(x[0].unsqueeze(0))\n",
    "        # i∈[1, N]\n",
    "        h0_2 = self.layer3(torch.cat((x[1:N+1], x[N+1:]), dim=1))\n",
    "        # i∈[N+1, 2N]\n",
    "        h0_3 = self.layer2(x[N+1:])\n",
    "        h0 = torch.vstack((h0_1, h0_2, h0_3))\n",
    "        return h0\n",
    "\n",
    "\n",
    "model1 = PairedContextEmbedding().to(device)\n",
    "print(model1)\n",
    "h0 = model1(node_information)\n",
    "print(h0.shape)\n",
    "print(h0[0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Context Encoding\n",
    "\n",
    "对节点信息做完嵌入操作后，参考Transformer的encoder部分对嵌入向量做encoding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 128])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "class ContextEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    ContextEncoding层\n",
    "    \"\"\"\n",
    "    def __init__(self, attention_nums=6):\n",
    "        super().__init__()\n",
    "        self.attention_nums = attention_nums\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=128, num_heads=8)\n",
    "        self.batch_normalization = nn.BatchNorm1d(num_features=128)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h0是执行完嵌入层后的向量 [7,128]\n",
    "        for _ in range(self.attention_nums):\n",
    "            ha, _ = self.mha(h, h, h)\n",
    "            hb = self.batch_normalization(h+ha)\n",
    "            h = self.batch_normalization(hb+self.feed_forward(hb))\n",
    "        return h\n",
    "\n",
    "model2 = ContextEncoding().to(device)\n",
    "h = model2(h0)\n",
    "print(h.shape)\n",
    "h_hat = h.mean(dim=0)\n",
    "print(h_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cooperative Multi-Agent Decoders\n",
    "\n",
    "每个智能体的decode网络其实就是actor网络，基于当前状态的观测，选择下一步要到达的节点 $v_{I^t_k}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0779, 0.5702, 0.0779, 0.0779, 0.0779, 0.0403, 0.0779]],\n",
      "       device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "AGENT_NUM = 2 # 智能体数\n",
    "\n",
    "class CooperativeDecoder(nn.Module):\n",
    "    def __init__(self, k, D=10, has_comm=False):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.D = D\n",
    "        self.has_comm = has_comm\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=128, num_heads=8)\n",
    "        self.W_C = nn.Linear(1, 128)\n",
    "        self.W_Q = nn.Linear(128, 128)\n",
    "        self.W_K = nn.Linear(128, 128)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.output = nn.Linear(3, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, state, h, h_hat, node_visit):\n",
    "        # state是所有智能体当前状态的矩阵，状态(h, C)，h表示隐藏态输出，C表示剩余容量\n",
    "        h_c = torch.cat([h_hat.unsqueeze(0), h[state[self.k][0]].unsqueeze(0), self.W_C(torch.tensor([state[self.k][1]], dtype=torch.float32, device=device).unsqueeze(0))])\n",
    "        if self.has_comm:\n",
    "            comm = torch.cat([h[state[k]][0], self.W_C(h[state[k]][1])] for k in range(AGENT_NUM))\n",
    "            h_c = torch.cat([h_c, comm])\n",
    "        g, _ = self.mha(h_c, h_c, h_c)\n",
    "        Q = self.W_Q(g)\n",
    "        K = self.W_K(h)\n",
    "        u = self.D * self.tanh(Q @ K.transpose(-1, -2) / np.sqrt(K.shape[0]))\n",
    "        u = self.output(u.transpose(-1, -2)).transpose(-1, -2)\n",
    "        # node_mask = -torch.inf * node_visit.unsqueeze(0).repeat(u.shape[0], 1)\n",
    "        node_mask = node_visit # 访问过为0，未访问过1\n",
    "        return F.softmax(node_mask*u, dim=1) # 选择下一个节点的概率值\n",
    "\n",
    "model3 = CooperativeDecoder(k=0).to(device)\n",
    "state = torch.tensor([\n",
    "    (np.random.randint(0, NODE_NUM), np.random.randint(0, 10)) for _ in range(AGENT_NUM)\n",
    "]).to(device)\n",
    "node_visit = torch.randint(0, 2, (NODE_NUM,)).to(device)\n",
    "action = model3(state, h, h_hat, node_visit)\n",
    "print(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
